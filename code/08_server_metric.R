# --- Setup ---
rm(list = ls())
library(tidyverse)
library(data.table)
library(cluster)
library(factoextra)
library(recipes)
library(pheatmap)
library(randomForest)
library(scales)
library(ggrepel)
library(gridExtra)

# --- Helper functions ---
compute_entropy <- function(x) {
    p <- prop.table(table(x))
    -sum(p * log2(p))
}
get_mode <- function(x) {
    ux <- unique(x)
    ux[which.max(tabulate(match(x, ux)))]
}

get_serve_profiles <- function(df, serve_numbers) {
    df %>%
        filter(ServeNumber %in% serve_numbers) %>%
        group_by(ServerName) %>%
        summarise(
            avg_speed = mean(Speed_MPH, na.rm = TRUE),
            sd_speed = sd(Speed_MPH, na.rm = TRUE),
            win_rate = mean(PointWinner == ServeIndicator, na.rm = TRUE),
            ace_pct = mean(is_ace, na.rm = TRUE),
            location_entropy = compute_entropy(location_bin),
            n_serves = n(),
            .groups = "drop"
        ) %>%
        filter(n_serves > 20)
}

run_pipeline <- function(df_clean, serve_label, output_dir) {
    set.seed(42)
    # --- Step 1: Get profiles ---
    profiles <- get_serve_profiles(df_clean, serve_label)
    
    # --- Step 2: Prep modeling data ---
    model_data <- profiles %>% select(-win_rate, -n_serves)
    
    recipe_obj <- recipe(~ ., data = model_data) %>%
        update_role(ServerName, new_role = "id") %>%
        step_center(all_numeric_predictors()) %>%
        step_scale(all_numeric_predictors()) %>%
        prep()
    
    df_encoded <- bake(recipe_obj, new_data = NULL) %>%
        select(-ServerName) %>%
        bind_cols(win_rate = profiles$win_rate,
                  ServerName = profiles$ServerName,
                  n_serves = profiles$n_serves)
    
    df_model <- df_encoded %>%
        column_to_rownames("ServerName") %>%
        select(-n_serves)
    
    # --- Step 3: Random Forest Model ---
    rf <- randomForest(win_rate ~ ., data = df_model, importance = TRUE)
    imp <- importance(rf, type = 1)
    imp_sorted <- sort(imp[, 1], decreasing = TRUE)
    rf_weights <- imp_sorted / sum(imp_sorted)
    serve_score_rf <- predict(rf, newdata = df_model)
    
    # --- Step 4: Compute weighted serve quality metric ---
    features <- names(rf_weights)
    df_model_scaled <- df_model %>%
        select(-win_rate) %>%
        mutate(across(all_of(features), ~ rescale(.x, to = c(0, 1)), .names = "rescaled_{.col}"))
    
    weighted_score <- df_model_scaled %>%
        rowwise() %>%
        mutate(serve_score_rf_weighted = sum(c_across(starts_with("rescaled_")) * rf_weights[features])) %>%
        ungroup() %>%
        mutate(ServerName = rownames(df_model),
               serve_score_rf = serve_score_rf[rownames(df_model)]) %>%
        select(ServerName, serve_score_rf, serve_score_rf_weighted)
    
    output_df <- bind_cols(weighted_score, profiles %>% select(-win_rate, -n_serves),
                           win_rate = profiles$win_rate,
                           n_serves = profiles$n_serves)
    
    write.csv(output_df, paste0(output_dir, "/server_quality.csv"), row.names = FALSE)
    
    # --- Step 5: Clustering visualization ---
    set.seed(42)
    kfit <- kmeans(df_model %>% select(-win_rate), centers = 3, nstart = 25)
    clustered <- df_model %>%
        select(-win_rate) %>%
        mutate(cluster = factor(kfit$cluster),
               ServerName = rownames(df_model)) %>%
        pivot_longer(cols = -c(cluster, ServerName), names_to = "Feature", values_to = "Value")
    
    p_feat <- ggplot(clustered, aes(x = cluster, y = Value, fill = cluster)) +
        geom_boxplot(outlier.size = 0.5) +
        facet_wrap(~ Feature, scales = "free", ncol = 3) +
        theme_minimal(base_size = 12) +
        theme(legend.position = "none") +
        labs(title = "Feature Distributions by Cluster", x = "Cluster", y = "Standardized Value")
    
    ggsave(paste0(output_dir, "/feature_distributions_by_cluster.png"), p_feat, width = 10, height = 8, bg = "white")
    
    # --- Step 6: RF variable importance plot ---
    imp_df <- data.frame(Variable = rownames(imp), Importance = imp[, 1])
    p_imp <- ggplot(imp_df, aes(x = reorder(Variable, Importance), y = Importance)) +
        geom_col(fill = "steelblue") +
        coord_flip() +
        theme_minimal(base_size = 12) +
        labs(title = "Random Forest Variable Importance", x = "Feature", y = "Importance")
    
    ggsave(paste0(output_dir, "/rf_variable_importance.png"), p_imp, width = 7, height = 5, bg = "white")
    
    # --- Step 7: Overperformance metric ---
    lm_model <- lm(win_rate ~ ace_pct + avg_speed + sd_speed + location_entropy, data = profiles)
    profiles$expected_win_rate <- predict(lm_model, newdata = profiles)
    profiles$overperformance <- profiles$win_rate - profiles$expected_win_rate
    
    overperf_summary <- profiles %>%
        select(ServerName, win_rate, expected_win_rate, overperformance, ace_pct, avg_speed, sd_speed, location_entropy, n_serves) %>%
        arrange(desc(overperformance))
    
    write.csv(overperf_summary, paste0(output_dir, "/server_overperformance.csv"), row.names = FALSE)
    
    p_over <- ggplot(overperf_summary, aes(x = expected_win_rate, y = win_rate)) +
        geom_point(aes(color = overperformance), size = 2.5) +
        geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray") +
        geom_text_repel(data = subset(overperf_summary, abs(overperformance) > 0.05),
                        aes(label = ServerName), size = 3) +
        scale_color_gradient2(low = "red", high = "green", mid = "gray90", midpoint = 0) +
        theme_minimal(base_size = 12) +
        labs(title = "Serve Overperformance vs Expected Win Rate",
             x = "Expected Win Rate", y = "Actual Win Rate", color = "Overperformance")
    
    ggsave(paste0(output_dir, "/serve_overperformance_plot.png"), p_over, width = 8, height = 6, bg = "white")
    
    # --- Return with ServerName column ---
    serve_quality_out <- output_df %>% rownames_to_column("ServerName")
    return(list(serve_quality = serve_quality_out))
}

# --- Load and clean training data ---
df_train <- fread("../data/processed/scaled/wimbledon_subset_m_training.csv")
df_clean <- df_train %>%
    filter(!is.na(ServeWidth), !is.na(ServeDepth), ServeWidth != "", ServeDepth != "") %>%
    filter(ServeNumber %in% c(1, 2)) %>%
    mutate(
        location_bin = paste0("W", ServeWidth, "_D", ServeDepth),
        ServerName = tolower(if_else(ServeIndicator == 1, player1, player2)),
        is_ace = if_else(ServeIndicator == 1, P1Ace, P2Ace)
    )

# --- Run all three pipelines ---
first_results <- run_pipeline(df_clean, 1, "../data/results/server_quality_models/first_serve")
second_results <- run_pipeline(df_clean, 2, "../data/results/server_quality_models/second_serve")
combined_results <- run_pipeline(df_clean, c(1, 2), "../data/results/server_quality_models/combined")

# --- Evaluate on testing set ---
df_test <- fread("../data/processed/scaled/wimbledon_subset_m_testing.csv")
df_test_clean <- df_test %>%
    filter(!is.na(ServeWidth), !is.na(ServeDepth), ServeWidth != "", ServeDepth != "") %>%
    filter(ServeNumber %in% c(1, 2)) %>%
    mutate(
        ServerName = tolower(if_else(ServeIndicator == 1, player1, player2)),
        won_point = PointWinner == ServeIndicator
    )

evaluate_on_test <- function(test_df, metric_df, metric_col) {
    df <- test_df %>%
        inner_join(metric_df %>% select(ServerName, !!sym(metric_col)), by = "ServerName")
    
    if (nrow(df) == 0) {
        return(tibble(
            cor_metric_vs_winrate = NA_real_,
            avg_actual_win_rate = NA_real_,
            avg_metric_score = NA_real_,
            n_players = 0
        ))
    }
    
    df %>%
        group_by(ServerName) %>%
        summarise(
            actual_win_rate = mean(won_point),
            metric_score = first(!!sym(metric_col)),
            .groups = "drop"
        ) %>%
        summarise(
            cor_metric_vs_winrate = cor(actual_win_rate, metric_score, use = "complete.obs"),
            avg_actual_win_rate = mean(actual_win_rate),
            avg_metric_score = mean(metric_score),
            n_players = n()
        )
}

first_eval <- evaluate_on_test(df_test_clean, first_results$serve_quality, "serve_score_rf_weighted")
second_eval <- evaluate_on_test(df_test_clean, second_results$serve_quality, "serve_score_rf_weighted")
combined_eval <- evaluate_on_test(df_test_clean, combined_results$serve_quality, "serve_score_rf_weighted")

comparison <- bind_rows(
    first_eval %>% mutate(Method = "First Serve"),
    second_eval %>% mutate(Method = "Second Serve"),
    combined_eval %>% mutate(Method = "Combined")
) %>%
    select(Method, everything())

print(comparison)
write.csv(comparison, "../data/results/server_quality_models/comparison/compare_all_overlap_test_performance.csv", row.names = FALSE)
